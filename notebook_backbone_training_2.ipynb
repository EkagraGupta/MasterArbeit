{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":8073860,"sourceType":"datasetVersion","datasetId":4764421},{"sourceId":91369,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":76598,"modelId":101255}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\ndataset_dir = '/kaggle/input/cifar-c'\nprint(os.listdir(dataset_dir))\n\n# Define the path to the subdirectory\nsub_dir = os.path.join(dataset_dir, 'CIFAR-10-C')\n\n# List contents of the subdirectory\nprint(os.listdir(sub_dir))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchmetrics\n!git clone https://github.com/EkagraGupta/MasterArbeit.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import statements\nimport torch\nimport random\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torchvision.transforms import TrivialAugmentWide\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import ConcatDataset\nimport os\n\n# Define the device\ntpu = True\n\nif tpu == True:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    device = xm.xla_device()\nelse:\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')  #intend here for else\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, np_images, original_dataset, resize):\n        # Load images\n        self.images = torch.from_numpy(np_images).permute(0, 3, 1, 2) / 255\n         #Normalize the images\n        #transform_test = transforms.Compose([\n            #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        #])\n        #self.images = transform_test(self.images)\n        #if resize == True:\n            #self.images = transforms.Resize(224, antialias=True)(self.images)\n        \n        # Extract labels from the original PyTorch dataset\n        self.labels = [label for _, label in original_dataset]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, index):\n        # Get image and label for the given index\n        image = self.images[index]\n        label = self.labels[index]\n\n        return image, label\n\n# Define the function to load corrupted datasets separately\ndef load_data_c_separately(dataset, testset, resize, test_transforms, batch_size):\n    corruptions = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression', 'speckle_noise', 'gaussian_blur', 'spatter', 'saturate']\n    np.asarray(corruptions)\n    c_datasets = {}\n    for corruption in corruptions:\n        if dataset == 'CIFAR10':\n            np_data_c = np.load(f'/kaggle/input/cifar-c/CIFAR-10-C/{corruption}.npy')\n            np_data_c = np.array(np.array_split(np_data_c, 5))\n            custom_dataset = CustomDataset(np_data_c[0], testset, resize)  # Load only one split for now\n            custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)\n            c_datasets[corruption] = custom_dataloader\n        else:\n            print('No corrupted benchmark available other than CIFAR10-c.')\n\n    return c_datasets\n\n\n# Load corrupted datasets\n#corrupted_datasets = load_data_c(dataset='CIFAR10', testset=testset, resize=True, \n                                 #test_transforms=None, subset=False, subsetsize=None)\n\n# Transformations for training and test sets\ntransform_train = transforms.Compose([\n    \n    transforms.RandomHorizontalFlip(),\n    transforms.RandomCrop(32, padding=4),  \n    TrivialAugmentWide(),\n    transforms.ToTensor(),\n])\ntransform_test = transforms.Compose([\n    transforms.ToTensor(),\n])\n\nbatch_size = 128\n\n# Use CIFAR-10 dataset for training\nbaseline_trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n                                        download=True, transform=transform_train)\nbaseline_trainloader = torch.utils.data.DataLoader(baseline_trainset, batch_size=batch_size,\n                                          shuffle=True, num_workers=2, pin_memory=True)\n\n# Use CIFAR-10 dataset for testing\nbaseline_testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n                                       download=True, transform=transform_test)\nbaseline_testloader = torch.utils.data.DataLoader(baseline_testset, batch_size=batch_size,\n                                         shuffle=False, num_workers=2, pin_memory=True)\n\n%cd /kaggle/working/MasterArbeit\nfrom augment_dataset import create_transforms, load_data\nfrom compute_loss import soft_loss\ntransforms_preprocess, transforms_augmentation = create_transforms(random_cropping=False, aggressive_augmentation=True, custom=True)\ncustom_trainset, custom_testset = load_data(transforms_preprocess=transforms_preprocess, transforms_augmentation=transforms_augmentation)\ncustom_trainloader = torch.utils.data.DataLoader(custom_trainset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\ncustom_testloader = torch.utils.data.DataLoader(custom_testset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n%cd\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(transforms_augmentation)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# functions to show an image\ndef imshow(img):\n    npimg = img.numpy()\n    plt.figure(figsize=(20, 25))  # Adjust the width and height as needed\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n\n# get some random training images\ndataiter = iter(custom_trainloader)\nimages, labels, confidences = next(dataiter)\n\n# if training baseline model, uncomment this\n# dataiter = iter(baseline_trainloader)\n# images, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n\n# print labels \nif isinstance(confidences, list):\n    # in case we have confidence \n    confidences = confidences[1]\n    \nprint(' '.join(f'{classes[labels[j]]:5s}: {confidences[j].item():.2f}' for j in range(batch_size)))\n# print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nimport numpy as np\n\n\n# Manual implementation of ResNet18\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(\n            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling layer\n        self.linear = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.avgpool(out)  # Apply global average pooling\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\n\n# ResNet18 Architecture\ndef ResNet18(num_classes=10):\n    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=True)\n\ndef conv_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        init.xavier_uniform(m.weight, gain=np.sqrt(2))\n        init.constant(m.bias, 0)\n    elif classname.find('BatchNorm') != -1:\n        init.constant(m.weight, 1)\n        init.constant(m.bias, 0)\n\nclass WideBasic(nn.Module):\n    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n        super(WideBasic, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, padding=1, bias=True)\n        self.dropout = nn.Dropout(p=dropout_rate)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=True),\n            )\n\n    def forward(self, x):\n        out = self.dropout(self.conv1(F.relu(self.bn1(x))))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += self.shortcut(x)\n\n        return out\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n                               stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion *\n                               planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth, widen_factor, dropout_rate=0.3, num_classes=10, factor=1, block=WideBasic):\n        super(WideResNet, self).__init__()\n        self.in_planes = 16\n\n        assert ((depth-4)%6 ==0), 'Wide-resnet depth should be 6n+4'\n        n = (int)((depth-4)/6)\n        k = widen_factor\n\n        nStages = [16, 16*k, 32*k, 64*k]\n\n        self.conv1 = conv3x3(3,nStages[0], stride=1)\n        self.layer1 = self._wide_layer(block, nStages[1], n, dropout_rate, stride=factor)\n        self.layer2 = self._wide_layer(block, nStages[2], n, dropout_rate, stride=2)\n        self.layer3 = self._wide_layer(block, nStages[3], n, dropout_rate, stride=2)\n        self.bn1 = nn.BatchNorm2d(nStages[3], momentum=0.9)\n        self.linear = nn.Linear(nStages[3], num_classes)\n\n    def _wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n            self.in_planes = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.relu(self.bn1(out))\n        out = F.avg_pool2d(out, 8)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n\n        return out\n\ndef WideResNet_28_4(num_classes, factor=1, block=WideBasic, dropout_rate=0.3):\n    return WideResNet(depth=28, widen_factor=4, dropout_rate=dropout_rate, num_classes=num_classes, factor=factor, block=block)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the model\nnet = WideResNet_28_4(num_classes=10)\nnet.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport time\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.15, momentum=0.9, weight_decay=1e-4)\n\n# Initialize the scheduler\nepochs = 100\nscheduler = CosineAnnealingLR(optimizer, T_max=epochs)  # Cosine Annealing LR Scheduler\n\n# Training loop\nprint(f'\\nStart Training...\\n')\nfor epoch in range(epochs):  # loop over the dataset multiple times\n    \n    start_time = time.time()\n    \n    running_loss = 0.0\n    total_train = 0\n    correct_train = 0\n    total = 0\n    correct = 0\n    \n    net.train()\n#     for i, (inputs, labels) in enumerate(baseline_trainloader):\n    for i, (inputs, labels, confidences) in enumerate(custom_trainloader):\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        \n        # when the model returns [augmentation_magnitude, confidence_aa], which is the case\n        # only when TA with soft labels is applied\n        if isinstance(confidences, list):\n            confidences = confidences[1]\n            \n        # get the inputs\n        inputs, labels, confidences = inputs.to(device), labels.to(device), confidences.to(device)\n            \n        # Convert labels to one-hot encoded vectors\n#         labels_one_hot = F.one_hot(labels, num_classes=10).float()\n        \n        # forward + backward + optimize\n        outputs = net(inputs)\n        \n#         hard_loss = criterion(outputs, labels)  \n        loss = soft_loss(pred=outputs, label=labels, confidence=confidences)\n#         print(f'Loss: {loss:.3f}\\tHard Loss: {hard_loss:.3f}')\n\n        loss.backward()\n        optimizer.step()\n        if tpu:\n            xm.mark_step()\n        running_loss += loss.item()\n\n        # Calculate training accuracy\n        _, predicted = torch.max(outputs.data, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n    \n    with torch.no_grad():\n        net.eval()\n        for images, labels in baseline_testloader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = net(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    \n    # calculate and print average loss for current epoch\n    average_loss = running_loss / len(custom_trainloader)\n    \n    print(f'\\nEpoch {epoch + 1} - Loss: {average_loss:.3f} - Test Accuracy: {100 * correct / total: .3f}')    \n    \n    scheduler.step()\n    end_time = time.time()\n    print(f'\\nProcessing time: {(end_time - start_time): 3f} seconds.')\n\nprint('Finished Training')\n\n# Save the trained model\nPATH = '/kaggle/working/cifar_net_exp04.pth'\ntorch.save(net.state_dict(), PATH)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Evaluate the model on Testset","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\n\n# Evaluate the CIFAR-10 dataset\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    net.eval()\n    for images, labels in baseline_testloader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    if total == 0:\n        print('No images found for CIFAR-10 dataset.')\n    else:\n        cifar10_accuracy = 100 * correct / total\n        print(f'Accuracy of the network on the CIFAR-10 test dataset: {cifar10_accuracy:.2f} %')\n\nif not tpu:\n    # Clear GPU memory\n    torch.cuda.empty_cache()\n\n    # Clear CPU memory\n    torch.cuda.ipc_collect()\n\n# Define the list of corruptions\ncorruptions = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', \n               'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', \n               'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression', \n               'speckle_noise', 'gaussian_blur', 'spatter', 'saturate']\n\n# Define the batch size\nbatch_size = 256 \n\n# Create an empty dictionary to store corrupted datasets\ncorrupted_datasets = {}\n\n# Prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n# List to store average accuracies for each corruption dataset\naverage_accuracies = []\n\n# Loop over corruptions, loading and testing all 5 severity levels of each corruption dataset\nfor corruption in corruptions:\n    print(f\"Testing on corruption: {corruption}\")\n\n    # Load and test datasets for all 5 severity levels of the current corruption\n    try:\n        np_data_c = np.load(f'/kaggle/input/cifar-c/CIFAR-10-C/{corruption}.npy')\n        np_data_c_splits = np.array_split(np_data_c, 5)\n        \n        # List to store accuracies of all severity levels for averaging\n        accuracies = []\n\n        for i, np_data_c_split in enumerate(np_data_c_splits):\n            custom_dataset = CustomDataset(np_data_c_split, baseline_testset, resize=True)\n            custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)\n\n            # Testing loop for the current corruption dataset split\n            correct = 0\n            total = 0\n\n            with torch.no_grad():\n                images_loaded = 0  # Counter for images loaded for the current corruption\n                for images, labels in custom_dataloader:\n                    images_loaded += len(images)  # Increment the counter by the number of images loaded\n                    images, labels = images.to(device), labels.to(device)\n                    # Calculate outputs by running images through the network\n                    outputs = net(images)\n                    # The class with the highest energy is chosen as prediction\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted == labels).sum().item()\n\n                if total == 0:\n                    print(f'No images found for {corruption} dataset split {i+1}.')\n                else:\n                    accuracy = 100 * correct / total\n                    print(f'Accuracy of the network on {corruption} dataset split {i+1}: {accuracy:.2f} %')\n                    accuracies.append(accuracy)\n\n                # Print the number of images loaded for the current corruption dataset split\n                #print(f\"Images loaded for {corruption} dataset split {i+1}: {images_loaded}\")\n                \n                if not tpu:\n                    # Clear GPU memory\n                    torch.cuda.empty_cache()\n                    # Clear CPU memory\n                    torch.cuda.ipc_collect()\n\n                # Delete variables to free up memory\n                del custom_dataset\n                del custom_dataloader\n\n        # Calculate and print the average accuracy for the corruption dataset\n        if accuracies:\n            average_accuracy = sum(accuracies) / len(accuracies)\n            average_accuracies.append(average_accuracy)\n            print(f'Average accuracy for {corruption} dataset: {average_accuracy:.2f} %')\n\n    except FileNotFoundError:\n        print(f'Corruption {corruption} dataset not found.')\n        continue\n\n# Calculate and print the average robust accuracy\nif average_accuracies:\n    average_robust_accuracy = sum(average_accuracies) / len(average_accuracies)\n    print(f'Average Robust Accuracy: {average_robust_accuracy:.2f} %')\nelse:\n    print(\"No corrupt datasets found for evaluation.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TA (soft) Trained (Latest: accuracy=95.60%)","metadata":{}},{"cell_type":"code","source":"# Load the model checkpoint\nnet = WideResNet_28_4(num_classes=10)\ncheckpoint_path = '/kaggle/working/MasterArbeit/models/cifar_net_exp003_tpu.pth'\ncheckpoint = torch.load(checkpoint_path)\nnet.load_state_dict(checkpoint, strict=False)\nnet.to(device)\n\n# Evaluate the CIFAR-10 dataset\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    net.eval()\n    for images, labels in baseline_testloader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    if total == 0:\n        print('No images found for CIFAR-10 dataset.')\n    else:\n        cifar10_accuracy = 100 * correct / total\n        print(f'Accuracy of the network on the CIFAR-10 test dataset: {cifar10_accuracy:.2f} %')\n\nif not tpu:\n    # Clear GPU memory\n    torch.cuda.empty_cache()\n\n    # Clear CPU memory\n    torch.cuda.ipc_collect()\n\n# Define the list of corruptions\ncorruptions = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', \n               'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', \n               'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression', \n               'speckle_noise', 'gaussian_blur', 'spatter', 'saturate']\n\n# Define the batch size\nbatch_size = 256 \n\n# Create an empty dictionary to store corrupted datasets\ncorrupted_datasets = {}\n\n# Prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n# List to store average accuracies for each corruption dataset\naverage_accuracies = []\n\n# Loop over corruptions, loading and testing all 5 severity levels of each corruption dataset\nfor corruption in corruptions:\n    print(f\"Testing on corruption: {corruption}\")\n\n    # Load and test datasets for all 5 severity levels of the current corruption\n    try:\n        np_data_c = np.load(f'/kaggle/input/cifar-c/CIFAR-10-C/{corruption}.npy')\n        np_data_c_splits = np.array_split(np_data_c, 5)\n        \n        # List to store accuracies of all severity levels for averaging\n        accuracies = []\n\n        for i, np_data_c_split in enumerate(np_data_c_splits):\n            custom_dataset = CustomDataset(np_data_c_split, baseline_testset, resize=True)\n            custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)\n\n            # Testing loop for the current corruption dataset split\n            correct = 0\n            total = 0\n\n            with torch.no_grad():\n                images_loaded = 0  # Counter for images loaded for the current corruption\n                for images, labels in custom_dataloader:\n                    images_loaded += len(images)  # Increment the counter by the number of images loaded\n                    images, labels = images.to(device), labels.to(device)\n                    # Calculate outputs by running images through the network\n                    outputs = net(images)\n                    # The class with the highest energy is chosen as prediction\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted == labels).sum().item()\n\n                if total == 0:\n                    print(f'No images found for {corruption} dataset split {i+1}.')\n                else:\n                    accuracy = 100 * correct / total\n                    print(f'Accuracy of the network on {corruption} dataset split {i+1}: {accuracy:.2f} %')\n                    accuracies.append(accuracy)\n\n                # Print the number of images loaded for the current corruption dataset split\n                #print(f\"Images loaded for {corruption} dataset split {i+1}: {images_loaded}\")\n                \n                if not tpu:\n                    # Clear GPU memory\n                    torch.cuda.empty_cache()\n                    # Clear CPU memory\n                    torch.cuda.ipc_collect()\n\n                # Delete variables to free up memory\n                del custom_dataset\n                del custom_dataloader\n\n        # Calculate and print the average accuracy for the corruption dataset\n        if accuracies:\n            average_accuracy = sum(accuracies) / len(accuracies)\n            average_accuracies.append(average_accuracy)\n            print(f'Average accuracy for {corruption} dataset: {average_accuracy:.2f} %')\n\n    except FileNotFoundError:\n        print(f'Corruption {corruption} dataset not found.')\n        continue\n\n# Calculate and print the average robust accuracy\nif average_accuracies:\n    average_robust_accuracy = sum(average_accuracies) / len(average_accuracies)\n    print(f'Average Robust Accuracy: {average_robust_accuracy:.2f} %')\nelse:\n    print(\"No corrupt datasets found for evaluation.\")\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Baseline Model (without TA)","metadata":{}},{"cell_type":"code","source":"# Load the model checkpoint\nnet = WideResNet_28_4(num_classes=10)\ncheckpoint_path = '/kaggle/working/MasterArbeit/models/cifar_net_baseline.pth'\ncheckpoint = torch.load(checkpoint_path)\nnet.load_state_dict(checkpoint, strict=False)\nnet.to(device)\n\n# Evaluate the CIFAR-10 dataset\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    net.eval()\n    for images, labels in baseline_testloader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    if total == 0:\n        print('No images found for CIFAR-10 dataset.')\n    else:\n        cifar10_accuracy = 100 * correct / total\n        print(f'Accuracy of the network on the CIFAR-10 test dataset: {cifar10_accuracy:.2f} %')\n\nif not tpu:\n    # Clear GPU memory\n    torch.cuda.empty_cache()\n\n    # Clear CPU memory\n    torch.cuda.ipc_collect()\n\n# Define the list of corruptions\ncorruptions = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', \n               'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', \n               'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression', \n               'speckle_noise', 'gaussian_blur', 'spatter', 'saturate']\n\n# Define the batch size\nbatch_size = 256 \n\n# Create an empty dictionary to store corrupted datasets\ncorrupted_datasets = {}\n\n# Prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n# List to store average accuracies for each corruption dataset\naverage_accuracies = []\n\n# Loop over corruptions, loading and testing all 5 severity levels of each corruption dataset\nfor corruption in corruptions:\n    print(f\"Testing on corruption: {corruption}\")\n\n    # Load and test datasets for all 5 severity levels of the current corruption\n    try:\n        np_data_c = np.load(f'/kaggle/input/cifar-c/CIFAR-10-C/{corruption}.npy')\n        np_data_c_splits = np.array_split(np_data_c, 5)\n        \n        # List to store accuracies of all severity levels for averaging\n        accuracies = []\n\n        for i, np_data_c_split in enumerate(np_data_c_splits):\n            custom_dataset = CustomDataset(np_data_c_split, baseline_testset, resize=True)\n            custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)\n\n            # Testing loop for the current corruption dataset split\n            correct = 0\n            total = 0\n\n            with torch.no_grad():\n                images_loaded = 0  # Counter for images loaded for the current corruption\n                for images, labels in custom_dataloader:\n                    images_loaded += len(images)  # Increment the counter by the number of images loaded\n                    images, labels = images.to(device), labels.to(device)\n                    # Calculate outputs by running images through the network\n                    outputs = net(images)\n                    # The class with the highest energy is chosen as prediction\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted == labels).sum().item()\n\n                if total == 0:\n                    print(f'No images found for {corruption} dataset split {i+1}.')\n                else:\n                    accuracy = 100 * correct / total\n                    print(f'Accuracy of the network on {corruption} dataset split {i+1}: {accuracy:.2f} %')\n                    accuracies.append(accuracy)\n\n                # Print the number of images loaded for the current corruption dataset split\n                #print(f\"Images loaded for {corruption} dataset split {i+1}: {images_loaded}\")\n                \n                if not tpu:\n                    # Clear GPU memory\n                    torch.cuda.empty_cache()\n                    # Clear CPU memory\n                    torch.cuda.ipc_collect()\n\n                # Delete variables to free up memory\n                del custom_dataset\n                del custom_dataloader\n\n        # Calculate and print the average accuracy for the corruption dataset\n        if accuracies:\n            average_accuracy = sum(accuracies) / len(accuracies)\n            average_accuracies.append(average_accuracy)\n            print(f'Average accuracy for {corruption} dataset: {average_accuracy:.2f} %')\n\n    except FileNotFoundError:\n        print(f'Corruption {corruption} dataset not found.')\n        continue\n\n# Calculate and print the average robust accuracy\nif average_accuracies:\n    average_robust_accuracy = sum(average_accuracies) / len(average_accuracies)\n    print(f'Average Robust Accuracy: {average_robust_accuracy:.2f} %')\nelse:\n    print(\"No corrupt datasets found for evaluation.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Baseline (TA trained)","metadata":{}},{"cell_type":"code","source":"# Load the model checkpoint\nnet = WideResNet_28_4(num_classes=10)\ncheckpoint_path = '/kaggle/working/MasterArbeit/models/cifar_net_ta_baseline.pth'\ncheckpoint = torch.load(checkpoint_path)\nnet.load_state_dict(checkpoint, strict=False)\nnet.to(device)\n\nimport torch\nimport numpy as np\n\n\n# Evaluate the CIFAR-10 dataset\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    net.eval()\n    for images, labels in baseline_testloader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    if total == 0:\n        print('No images found for CIFAR-10 dataset.')\n    else:\n        cifar10_accuracy = 100 * correct / total\n        print(f'Accuracy of the network on the CIFAR-10 test dataset: {cifar10_accuracy:.2f} %')\n\nif not tpu:\n    # Clear GPU memory\n    torch.cuda.empty_cache()\n\n    # Clear CPU memory\n    torch.cuda.ipc_collect()\n\n# Define the list of corruptions\ncorruptions = ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', \n               'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', \n               'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression', \n               'speckle_noise', 'gaussian_blur', 'spatter', 'saturate']\n\n# Define the batch size\nbatch_size = 256 \n\n# Create an empty dictionary to store corrupted datasets\ncorrupted_datasets = {}\n\n# Prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in classes}\ntotal_pred = {classname: 0 for classname in classes}\n\n# List to store average accuracies for each corruption dataset\naverage_accuracies = []\n\n# Loop over corruptions, loading and testing all 5 severity levels of each corruption dataset\nfor corruption in corruptions:\n    print(f\"Testing on corruption: {corruption}\")\n\n    # Load and test datasets for all 5 severity levels of the current corruption\n    try:\n        np_data_c = np.load(f'/kaggle/input/cifar-c/CIFAR-10-C/{corruption}.npy')\n        np_data_c_splits = np.array_split(np_data_c, 5)\n        \n        # List to store accuracies of all severity levels for averaging\n        accuracies = []\n\n        for i, np_data_c_split in enumerate(np_data_c_splits):\n            custom_dataset = CustomDataset(np_data_c_split, baseline_testset, resize=True)\n            custom_dataloader = torch.utils.data.DataLoader(custom_dataset, batch_size=batch_size, shuffle=False)\n\n            # Testing loop for the current corruption dataset split\n            correct = 0\n            total = 0\n\n            with torch.no_grad():\n                images_loaded = 0  # Counter for images loaded for the current corruption\n                for images, labels in custom_dataloader:\n                    images_loaded += len(images)  # Increment the counter by the number of images loaded\n                    images, labels = images.to(device), labels.to(device)\n                    # Calculate outputs by running images through the network\n                    outputs = net(images)\n                    # The class with the highest energy is chosen as prediction\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted == labels).sum().item()\n\n                if total == 0:\n                    print(f'No images found for {corruption} dataset split {i+1}.')\n                else:\n                    accuracy = 100 * correct / total\n                    print(f'Accuracy of the network on {corruption} dataset split {i+1}: {accuracy:.2f} %')\n                    accuracies.append(accuracy)\n\n                # Print the number of images loaded for the current corruption dataset split\n                #print(f\"Images loaded for {corruption} dataset split {i+1}: {images_loaded}\")\n                \n                if not tpu:\n                    # Clear GPU memory\n                    torch.cuda.empty_cache()\n                    # Clear CPU memory\n                    torch.cuda.ipc_collect()\n\n                # Delete variables to free up memory\n                del custom_dataset\n                del custom_dataloader\n\n        # Calculate and print the average accuracy for the corruption dataset\n        if accuracies:\n            average_accuracy = sum(accuracies) / len(accuracies)\n            average_accuracies.append(average_accuracy)\n            print(f'Average accuracy for {corruption} dataset: {average_accuracy:.2f} %')\n\n    except FileNotFoundError:\n        print(f'Corruption {corruption} dataset not found.')\n        continue\n\n# Calculate and print the average robust accuracy\nif average_accuracies:\n    average_robust_accuracy = sum(average_accuracies) / len(average_accuracies)\n    print(f'Average Robust Accuracy: {average_robust_accuracy:.2f} %')\nelse:\n    print(\"No corrupt datasets found for evaluation.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}